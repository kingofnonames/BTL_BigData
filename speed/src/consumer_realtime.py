import json
import logging
import time
import sqlite3
from datetime import datetime
import pandas as pd

try:
    from kafka import KafkaConsumer
except ImportError:
    print(" Lá»—i: ChÆ°a cÃ i Ä‘áº·t thÆ° viá»‡n kafka-python.")
    exit()

# Cáº¤U HÃŒNH
KAFKA_TOPIC = 'stock_realtime_data'
KAFKA_BOOTSTRAP_SERVERS = ['localhost:9092']
DB_NAME = 'stock_data.db' # TÃªn file database

logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')
logger = logging.getLogger(__name__)

def init_db():
    """Khá»Ÿi táº¡o database SQLite vÃ  báº£ng dá»¯ liá»‡u"""
    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()
    # Táº¡o báº£ng náº¿u chÆ°a cÃ³. LÆ°u Ã½: DÃ¹ng ticker lÃ m khÃ³a chÃ­nh Ä‘á»ƒ update giÃ¡ má»›i nháº¥t
    c.execute('''
        CREATE TABLE IF NOT EXISTS realtime_prices (
            ticker TEXT PRIMARY KEY,
            price REAL,
            volume INTEGER,
            time TEXT,
            ingestion_time TEXT,
            match_type TEXT
        )
    ''')
    conn.commit()
    conn.close()
    logger.info(f" ÄÃ£ khá»Ÿi táº¡o Database: {DB_NAME}")

def save_to_sqlite(df_batch):
    """LÆ°u batch dá»¯ liá»‡u vÃ o SQLite"""
    if df_batch.empty:
        return

    conn = sqlite3.connect(DB_NAME)
    try:
        # Sá»­ dá»¥ng to_sql khÃ´ng há»— trá»£ UPSERT (Update if exists) tá»‘t trong pandas cÅ©
        # NÃªn ta dÃ¹ng executeMany cá»§a sqlite3 Ä‘á»ƒ tá»‘i Æ°u
        
        data_to_insert = []
        for _, row in df_batch.iterrows():
            data_to_insert.append((
                row.get('ticker'),
                row.get('price'),
                row.get('volume'),
                str(row.get('time')),
                str(row.get('ingestion_time')),
                row.get('match_type')
            ))

        # CÃ¢u lá»‡nh SQL: INSERT OR REPLACE (Náº¿u mÃ£ Ä‘Ã£ cÃ³ thÃ¬ ghi Ä‘Ã¨ giÃ¡ má»›i nháº¥t)
        conn.executemany('''
            INSERT OR REPLACE INTO realtime_prices (ticker, price, volume, time, ingestion_time, match_type)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', data_to_insert)
        
        conn.commit()
        logger.info(f" ÄÃ£ lÆ°u {len(df_batch)} báº£n ghi vÃ o DB.")
    except Exception as e:
        logger.error(f"Lá»—i lÆ°u DB: {e}")
    finally:
        conn.close()

def run_consumer():
    init_db() # Táº¡o DB trÆ°á»›c khi cháº¡y
    
    logger.info("ðŸŽ¬ Äang khá»Ÿi Ä‘á»™ng Consumer...")
    
    consumer = KafkaConsumer(
        KAFKA_TOPIC,
        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
        auto_offset_reset='latest',
        enable_auto_commit=True,
        group_id='stock-db-saver-group',
        value_deserializer=lambda x: json.loads(x.decode('utf-8'))
    )
    
    logger.info(f"âœ… ÄÃ£ káº¿t ná»‘i Topic '{KAFKA_TOPIC}'.")
    
    batch_data = []
    last_process_time = time.time()
    
    for message in consumer:
        record = message.value
        record['ingestion_time'] = datetime.now().isoformat()
        batch_data.append(record)
        
        # Xá»­ lÃ½ theo Batch (má»—i 5 báº£n ghi hoáº·c 2 giÃ¢y) Ä‘á»ƒ giáº£m táº£i IO Database
        current_time = time.time()
        if len(batch_data) >= 5 or (current_time - last_process_time > 2 and len(batch_data) > 0):
            
            df = pd.DataFrame(batch_data)
            
            # 1. In ra mÃ n hÃ¬nh Ä‘á»ƒ debug
            print(f"\n--- NHáº¬N {len(batch_data)} MÃƒ LÃšC {datetime.now().strftime('%H:%M:%S')} ---")
            print(df[['ticker', 'price', 'time']].head(3).to_string(index=False)) # Chá»‰ in 3 dÃ²ng Ä‘áº§u
            
            # 2. LÆ°u vÃ o Database (Serving Layer)
            save_to_sqlite(df)
            
            batch_data = []
            last_process_time = current_time

if __name__ == "__main__":
    run_consumer()
