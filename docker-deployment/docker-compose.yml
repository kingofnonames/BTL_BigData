version: "3.8"

networks:
  bigdata-net:

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    networks: [bigdata-net]
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    mem_limit: 256m
    cpus: 0.3

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    networks: [bigdata-net]
    depends_on: [zookeeper]
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    mem_limit: 768m
    cpus: 0.8

  # =====================
  # HDFS NAMENODE
  # =====================
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    networks: [bigdata-net]
    environment:
      CLUSTER_NAME: lambda-cluster
    volumes:
      - ./data/hdfs/namenode:/hadoop/dfs/name
    ports:
      - "9870:9870"
    mem_limit: 1g
    cpus: 0.7

  # =====================
  # HDFS DATANODE
  # =====================
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    networks: [bigdata-net]
    depends_on: [namenode]
    environment:
      CLUSTER_NAME: lambda-cluster
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
    volumes:
      - ./data/hdfs/datanode:/hadoop/dfs/data
    mem_limit: 1g
    cpus: 0.7

  # =====================
  # SPARK MASTER
  # =====================
  spark-master:
    image: apache/spark:3.5.1
    container_name: spark-master
    networks: [bigdata-net]
    command: >
      /opt/spark/bin/spark-class
      org.apache.spark.deploy.master.Master
    ports:
      - "7077:7077"
      - "8081:8080"
    volumes:
      - ./spark/jobs:/opt/spark/jobs
    mem_limit: 512m
    cpus: 0.4

  # =====================
  # SPARK WORKER
  # =====================
  spark-worker:
    image: apache/spark:3.5.1
    container_name: spark-worker
    networks: [bigdata-net]
    depends_on: [spark-master]
    command: >
      /opt/spark/bin/spark-class
      org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
    environment:
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 1g
    mem_limit: 1.5g
    cpus: 1.0

  # =====================
  # ELASTICSEARCH
  # =====================
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    networks: [bigdata-net]
    environment:
      discovery.type: single-node
      xpack.security.enabled: "false"
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - ./data/elasticsearch:/usr/share/elasticsearch/data
    mem_limit: 1g
    cpus: 0.8

  # =====================
  # KIBANA
  # =====================
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    networks: [bigdata-net]
    depends_on: [elasticsearch]
    ports:
      - "5601:5601"
    mem_limit: 1g
    cpus: 0.3

  # =====================
  # AIRFLOW DATABASE
  # =====================
  airflow-db:
    image: postgres:13
    container_name: airflow-db
    networks: [bigdata-net]
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - ./data/airflow-db:/var/lib/postgresql/data
    mem_limit: 512m
    cpus: 0.4

  # =====================
  # AIRFLOW WEBSERVER
  # =====================
  airflow-webserver:
    image: apache/airflow:2.8.1
    container_name: airflow-webserver
    networks: [bigdata-net]
    depends_on: [airflow-db]
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
    ports:
      - "8080:8080"
    command: webserver
    mem_limit: 768m
    cpus: 0.6

  # =====================
  # AIRFLOW SCHEDULER
  # =====================
  airflow-scheduler:
    image: apache/airflow:2.8.1
    container_name: airflow-scheduler
    networks: [bigdata-net]
    depends_on: [airflow-webserver]
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    command: scheduler
    mem_limit: 512m
    cpus: 0.4
